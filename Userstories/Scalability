# Detailed Solutions for Typosentinel Scalability & Architecture

## 1. Scalability Solutions

### Horizontal Scaling Architecture
```mermaid
graph TD
    A[Load Balancer] --> B1[Go Backend 1]
    A --> B2[Go Backend 2] 
    A --> B3[Go Backend N]
    
    B1 --> Q[Message Queue<br/>Redis/RabbitMQ]
    B2 --> Q
    B3 --> Q
    
    Q --> W1[Worker Pool 1]
    Q --> W2[Worker Pool 2]
    Q --> W3[Worker Pool N]
    
    W1 --> ML1[ML Service 1]
    W2 --> ML2[ML Service 2]
    W3 --> ML3[ML Service N]
    
    B1 --> DB[(Primary DB)]
    B2 --> DB
    B3 --> DB
    
    DB --> R1[(Read Replica 1)]
    DB --> R2[(Read Replica 2)]
```

### Queue-Based Processing System
```go
// internal/queue/scanner_queue.go
type ScanRequest struct {
    PackageID    string            `json:"package_id"`
    PackageName  string            `json:"package_name"`
    Registry     string            `json:"registry"`
    Priority     Priority          `json:"priority"`
    Metadata     map[string]string `json:"metadata"`
    RequestedBy  string            `json:"requested_by"`
    Timestamp    time.Time         `json:"timestamp"`
}

type ScannerQueue struct {
    redis    *redis.Client
    workers  int
    ctx      context.Context
    cancel   context.CancelFunc
}

func (sq *ScannerQueue) EnqueueScan(req *ScanRequest) error {
    // Priority queues: critical, high, normal, low
    queueName := fmt.Sprintf("scan_queue:%s", req.Priority)
    
    data, err := json.Marshal(req)
    if err != nil {
        return err
    }
    
    return sq.redis.LPush(sq.ctx, queueName, data).Err()
}

func (sq *ScannerQueue) StartWorkers() {
    for i := 0; i < sq.workers; i++ {
        go sq.worker(i)
    }
}

func (sq *ScannerQueue) worker(id int) {
    queues := []string{"scan_queue:critical", "scan_queue:high", "scan_queue:normal", "scan_queue:low"}
    
    for {
        select {
        case <-sq.ctx.Done():
            return
        default:
            // BRPOP with timeout for graceful shutdown
            result, err := sq.redis.BRPop(sq.ctx, 5*time.Second, queues...).Result()
            if err == redis.Nil {
                continue
            }
            if err != nil {
                log.Printf("Worker %d: Queue error: %v", id, err)
                continue
            }
            
            var req ScanRequest
            if err := json.Unmarshal([]byte(result[1]), &req); err != nil {
                log.Printf("Worker %d: Unmarshal error: %v", id, err)
                continue
            }
            
            sq.processScan(id, &req)
        }
    }
}
```

### Batch Processing for Large Organizations
```go
// internal/batch/batch_processor.go
type BatchProcessor struct {
    scanner     *scanner.Scanner
    db          *database.DB
    concurrency int
}

func (bp *BatchProcessor) ProcessPackageList(packages []string, orgID string) error {
    // Create batch job record
    batch := &types.BatchJob{
        ID:           uuid.New().String(),
        OrganizationID: orgID,
        TotalPackages: len(packages),
        Status:       "running",
        CreatedAt:    time.Now(),
    }
    
    if err := bp.db.CreateBatchJob(batch); err != nil {
        return err
    }
    
    // Process in chunks with rate limiting
    sem := make(chan struct{}, bp.concurrency)
    var wg sync.WaitGroup
    
    for i, pkg := range packages {
        wg.Add(1)
        go func(packageName string, index int) {
            defer wg.Done()
            sem <- struct{}{} // Acquire semaphore
            defer func() { <-sem }() // Release semaphore
            
            result, err := bp.scanner.Scan(context.Background(), &types.Package{
                Name: packageName,
            })
            
            // Update batch progress
            bp.updateBatchProgress(batch.ID, index+1, result, err)
        }(pkg, i)
    }
    
    wg.Wait()
    bp.finalizeBatch(batch.ID)
    return nil
}
```

## 2. Database Strategy

### Hybrid Database Architecture
```yaml
# docker-compose.yml database services
services:
  # Primary PostgreSQL for structured data
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: typosentinel
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"

  # Redis for caching and queues
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # MinIO for blob storage (package contents, reports)
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  # ClickHouse for analytics and time-series data
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    ports:
      - "8123:8123"
      - "9001:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
```

### Database Schema Design
```sql
-- PostgreSQL schema for core data
-- migrations/001_initial_schema.sql

-- Organizations and users
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    settings JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Packages and scan results
CREATE TABLE packages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    registry VARCHAR(100) NOT NULL,
    version VARCHAR(100),
    package_url TEXT, -- PURL format
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(name, registry, version)
);

CREATE TABLE scan_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    package_id UUID REFERENCES packages(id),
    organization_id UUID REFERENCES organizations(id),
    scan_type VARCHAR(50) NOT NULL, -- 'manual', 'scheduled', 'ci'
    overall_risk VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    risk_score DECIMAL(5,2) NOT NULL,
    findings JSONB NOT NULL DEFAULT '[]',
    metadata JSONB DEFAULT '{}',
    scan_duration_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_scan_results_org_created ON scan_results(organization_id, created_at DESC);
CREATE INDEX idx_scan_results_risk ON scan_results(overall_risk, risk_score DESC);
CREATE INDEX idx_packages_name_registry ON packages(name, registry);
```

### Data Access Layer
```go
// internal/database/repository.go
type Repository struct {
    db          *sql.DB
    redis       *redis.Client
    blobStorage *minio.Client
    clickhouse  *sql.DB
}

// Cached package lookup
func (r *Repository) GetPackage(name, registry string) (*types.Package, error) {
    cacheKey := fmt.Sprintf("package:%s:%s", name, registry)
    
    // Try cache first
    cached, err := r.redis.Get(context.Background(), cacheKey).Result()
    if err == nil {
        var pkg types.Package
        if json.Unmarshal([]byte(cached), &pkg) == nil {
            return &pkg, nil
        }
    }
    
    // Fallback to database
    var pkg types.Package
    err = r.db.QueryRow(`
        SELECT id, name, registry, version, package_url, created_at 
        FROM packages 
        WHERE name = $1 AND registry = $2
        ORDER BY created_at DESC 
        LIMIT 1
    `, name, registry).Scan(&pkg.ID, &pkg.Name, &pkg.Registry, &pkg.Version, &pkg.PackageURL, &pkg.CreatedAt)
    
    if err != nil {
        return nil, err
    }
    
    // Cache for 1 hour
    data, _ := json.Marshal(pkg)
    r.redis.Set(context.Background(), cacheKey, data, time.Hour)
    
    return &pkg, nil
}

// Store large scan artifacts in blob storage
func (r *Repository) StoreScanArtifacts(scanID string, artifacts map[string][]byte) error {
    bucketName := "scan-artifacts"
    
    for filename, data := range artifacts {
        objectName := fmt.Sprintf("%s/%s", scanID, filename)
        
        _, err := r.blobStorage.PutObject(
            context.Background(),
            bucketName,
            objectName,
            bytes.NewReader(data),
            int64(len(data)),
            minio.PutObjectOptions{ContentType: "application/octet-stream"},
        )
        if err != nil {
            return err
        }
    }
    
    return nil
}
```

## 3. Real-time vs Batch Processing

### Event-Driven Architecture
```go
// internal/events/event_system.go
type EventType string

const (
    PackageScanned     EventType = "package.scanned"
    ThreatDetected     EventType = "threat.detected"
    BatchCompleted     EventType = "batch.completed"
    PolicyViolation    EventType = "policy.violation"
)

type Event struct {
    ID        string                 `json:"id"`
    Type      EventType              `json:"type"`
    Source    string                 `json:"source"`
    Data      map[string]interface{} `json:"data"`
    Timestamp time.Time              `json:"timestamp"`
}

type EventBus struct {
    redis      *redis.Client
    handlers   map[EventType][]EventHandler
    mu         sync.RWMutex
}

func (eb *EventBus) Publish(event *Event) error {
    data, err := json.Marshal(event)
    if err != nil {
        return err
    }
    
    // Publish to Redis streams for persistence and replay
    return eb.redis.XAdd(context.Background(), &redis.XAddArgs{
        Stream: fmt.Sprintf("events:%s", event.Type),
        Values: map[string]interface{}{
            "data": string(data),
        },
    }).Err()
}

func (eb *EventBus) Subscribe(eventType EventType, handler EventHandler) {
    eb.mu.Lock()
    defer eb.mu.Unlock()
    
    if eb.handlers[eventType] == nil {
        eb.handlers[eventType] = make([]EventHandler, 0)
    }
    eb.handlers[eventType] = append(eb.handlers[eventType], handler)
}
```

### WebSocket Integration for Real-time Updates
```typescript
// web/src/hooks/useRealTimeUpdates.ts
export const useRealTimeUpdates = (organizationId: string) => {
  const [scanResults, setScanResults] = useState<ScanResult[]>([]);
  const [threats, setThreats] = useState<Threat[]>([]);
  const socketRef = useRef<WebSocket | null>(null);

  useEffect(() => {
    const wsUrl = `${process.env.REACT_APP_WS_URL}/ws/${organizationId}`;
    socketRef.current = new WebSocket(wsUrl);

    socketRef.current.onmessage = (event) => {
      const message = JSON.parse(event.data);
      
      switch (message.type) {
        case 'scan_completed':
          setScanResults(prev => [message.data, ...prev.slice(0, 99)]); // Keep last 100
          break;
        case 'threat_detected':
          setThreats(prev => [message.data, ...prev]);
          // Show notification
          toast.error(`High-risk package detected: ${message.data.packageName}`);
          break;
        case 'batch_progress':
          // Update batch progress UI
          updateBatchProgress(message.data);
          break;
      }
    };

    return () => {
      socketRef.current?.close();
    };
  }, [organizationId]);

  return { scanResults, threats };
};
```

## 4. Detection Accuracy Solutions

### Multi-Layer ML Pipeline
```python
# ml/models/ensemble_detector.py
class EnsembleDetector:
    def __init__(self):
        self.similarity_model = SemanticSimilarityModel()
        self.malicious_classifier = MaliciousPackageClassifier()
        self.behavioral_analyzer = BehavioralAnalyzer()
        self.reputation_scorer = ReputationScorer()
        
    def analyze_package(self, package_data: PackageData) -> ThreatAssessment:
        # 1. Semantic similarity analysis
        similarity_scores = self.similarity_model.find_similar(
            package_data.name, 
            threshold=0.7
        )
        
        # 2. Static code analysis
        static_features = self.extract_static_features(package_data)
        malicious_prob = self.malicious_classifier.predict_proba(static_features)
        
        # 3. Behavioral analysis
        behavioral_score = self.behavioral_analyzer.analyze(package_data)
        
        # 4. Reputation scoring
        reputation_score = self.reputation_scorer.score(package_data)
        
        # Ensemble combination with weights learned from validation data
        final_score = self.combine_scores(
            similarity_scores, malicious_prob, 
            behavioral_score, reputation_score
        )
        
        return ThreatAssessment(
            risk_score=final_score,
            confidence=self.calculate_confidence(final_score),
            contributing_factors=self.explain_decision(final_score),
            recommendations=self.generate_recommendations(final_score)
        )

# Feature engineering for better accuracy
class FeatureExtractor:
    def extract_comprehensive_features(self, package_data: PackageData) -> Dict:
        features = {}
        
        # Name-based features
        features.update(self.extract_name_features(package_data.name))
        
        # Metadata features
        features.update(self.extract_metadata_features(package_data.metadata))
        
        # Code analysis features
        if package_data.source_code:
            features.update(self.extract_code_features(package_data.source_code))
        
        # Network/dependency features
        features.update(self.extract_dependency_features(package_data.dependencies))
        
        return features
    
    def extract_name_features(self, name: str) -> Dict:
        return {
            'name_length': len(name),
            'has_numbers': bool(re.search(r'\d', name)),
            'has_special_chars': bool(re.search(r'[^a-zA-Z0-9\-_.]', name)),
            'entropy': self.calculate_entropy(name),
            'keyboard_patterns': self.detect_keyboard_patterns(name),
            'common_typo_patterns': self.detect_typo_patterns(name)
        }
```

### Continuous Learning System
```python
# ml/training/continuous_learning.py
class ContinuousLearner:
    def __init__(self):
        self.model_store = ModelStore()
        self.feedback_collector = FeedbackCollector()
        
    async def retrain_models(self):
        """Retrain models with new feedback data"""
        # Collect feedback from human analysts
        feedback_data = await self.feedback_collector.get_new_feedback()
        
        if len(feedback_data) < MIN_FEEDBACK_THRESHOLD:
            return
        
        # Retrain similarity model
        similarity_model = self.model_store.load_model('similarity')
        retrained_similarity = self.retrain_similarity_model(
            similarity_model, feedback_data
        )
        
        # Validate on holdout set
        if self.validate_model(retrained_similarity) > PERFORMANCE_THRESHOLD:
            self.model_store.save_model('similarity', retrained_similarity)
            
        # Similar process for other models...
        
    def validate_model(self, model) -> float:
        """Validate model performance on test set"""
        test_data = self.load_test_data()
        predictions = model.predict(test_data.features)
        
        # Calculate metrics
        precision = precision_score(test_data.labels, predictions)
        recall = recall_score(test_data.labels, predictions)
        f1 = f1_score(test_data.labels, predictions)
        
        return f1  # Use F1 as primary metric
```

### Rule-Based Static Analysis Enhancement
```go
// internal/analysis/static/rule_engine.go
type RuleEngine struct {
    rules []Rule
}

type Rule interface {
    Name() string
    Severity() Severity
    Check(pkg *types.Package, code []byte) (*Finding, error)
}

// Example rule: Detect suspicious network calls
type SuspiciousNetworkRule struct{}

func (r *SuspiciousNetworkRule) Check(pkg *types.Package, code []byte) (*Finding, error) {
    suspiciousPatterns := []string{
        `requests\.get\(['"](https?://(?!pypi\.org|npmjs\.com|github\.com))`,
        `urllib\.request\.urlopen\(['"](https?://(?!pypi\.org|npmjs\.com))`,
        `fetch\(['"](https?://(?!npmjs\.com|github\.com))`,
    }
    
    var findings []string
    for _, pattern := range suspiciousPatterns {
        re := regexp.MustCompile(pattern)
        matches := re.FindAllString(string(code), -1)
        findings = append(findings, matches...)
    }
    
    if len(findings) > 0 {
        return &Finding{
            Rule:        r.Name(),
            Severity:    High,
            Message:     "Package makes suspicious network requests",
            Evidence:    findings,
            Confidence:  0.8,
        }, nil
    }
    
    return nil, nil
}
```

This comprehensive approach ensures Typosentinel can scale effectively while maintaining high detection accuracy and providing both real-time monitoring and batch processing capabilities for different organizational needs.